{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d6a3118-08de-490b-adde-ba496e5c07c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "# Install Poppler using apt-get (for Debian-based distributions)\n",
    "subprocess.run([\"sudo\", \"apt-get\", \"update\"])\n",
    "subprocess.run([\"sudo\", \"apt-get\", \"install\", \"-y\", \"poppler-utils\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "664434b1-ca22-48ab-8ad5-3a86aa32db00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"EKIH/Includes/Configurations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f415434c-8eb9-475c-8686-0c5beff92af9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"EKIH/Includes/DBUtils\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92ffbecd-3110-4aef-aede-7f36bd7bfa61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import magic\n",
    "import fitz\n",
    "import io\n",
    "from io import BytesIO, StringIO\n",
    "import traceback\n",
    "import hashlib \n",
    "import magic\n",
    "import aspose.words as aw\n",
    "#import aspose.slides as slides\n",
    "from azure.storage.filedatalake import DataLakeFileClient, ContentSettings, DataLakeServiceClient\n",
    "import aspose.pydrawing as drawing\n",
    "import subprocess\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "import pdf2image\n",
    "from pdf2image import convert_from_bytes\n",
    "import sys\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ba3e239-723e-45da-8a93-6f8ba0461c54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"OPU\",\"\")\n",
    "opu = dbutils.widgets.get(\"OPU\")\n",
    "\n",
    "dbutils.widgets.text(\"Source\",\"\")\n",
    "source = dbutils.widgets.get(\"Source\")\n",
    "\n",
    "dbutils.widgets.text(\"Doc_Type\",\"\")\n",
    "docType = dbutils.widgets.get(\"Doc_Type\")\n",
    "\n",
    "dbutils.widgets.text(\"Pipeline_RunID\",\"\")\n",
    "pipelineRunID = dbutils.widgets.get(\"Pipeline_RunID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eeb9c8d7-31aa-4b06-b22a-6302700ba1b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Oscar's detecting of Scan/Generated Documents\n",
    "\n",
    "def PDFPageCheck(raw_content):\n",
    "    doc = fitz.open(stream=raw_content,filetype='pdf')\n",
    "    doc_len = len(doc)\n",
    "\n",
    "    if doc_len > 5:\n",
    "        last = 5\n",
    "    else:\n",
    "        last = doc_len\n",
    "\n",
    "    image_width=[]\n",
    "    image_height=[]\n",
    "    result = []\n",
    "    img_count_pdf = 0\n",
    "    #get the size of image inside a page\n",
    "    for current_page_index in range(last):\n",
    "        page_img_count = 0\n",
    "        for _ ,img in enumerate(doc.get_page_images(current_page_index)):\n",
    "            xref = img[0]\n",
    "            image = fitz.Pixmap(doc, xref)\n",
    "            if image.n >= 4:   #5                  \n",
    "                image = fitz.Pixmap(fitz.csRGB, image)\n",
    "            image_bytes = image.tobytes()\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            image_width.append(image.width)\n",
    "            image_height.append(image.height)\n",
    "            page_img_count+=1\n",
    "            img_count_pdf+=1\n",
    "            del image_bytes\n",
    "            \n",
    "        if page_img_count > 1:\n",
    "            result.append(\"This page may be a generated PDF.\")\n",
    "            ans = 'yes'\n",
    "\n",
    "    if img_count_pdf < last:\n",
    "        result.append(\"This page may be a generated PDF.\")\n",
    "        ans = 'yes'\n",
    "\n",
    "    #if no more than 1 images extracted in a page then will proceed to this\n",
    "    if len(result) > 0:\n",
    "        ans_gen = isGeneratedPDF(result,doc_len,ans)\n",
    "    else:\n",
    "        #convert the pdf pages into images then get size of the img\n",
    "        images = convert_from_bytes(raw_content, first_page=1, last_page=last)\n",
    "        page_width=[]\n",
    "        page_height=[]\n",
    "        for i, image in enumerate(images):\n",
    "            image_bytes = BytesIO()\n",
    "            image.save(image_bytes, format=\"JPEG\")\n",
    "            page_width.append(image.width)\n",
    "            page_height.append(image.height)\n",
    "            del image_bytes  \n",
    "\n",
    "        for i in range (last):\n",
    "            answer = (is_plus_minus_2(image_width[i],page_width[i],image_height[i],page_height[i]))\n",
    "            result.append(answer)\n",
    "\n",
    "        ans = 'no'\n",
    "        #ans_gen = isGeneratedPDF(result,len(doc),ans)\n",
    "    return result,doc_len,ans\n",
    "\n",
    "def getCaption(fullText):  ##To get image caption, 'Fig or Figure'\n",
    "    captionText = fullText.splitlines()\n",
    "    running_num = 1\n",
    "    caption=[]\n",
    "    for row in captionText:\n",
    "        if('Figure' in row[:6] or 'Fig.' in row[:4]):\n",
    "            caption.append(str(running_num)+'. : '+row)\n",
    "            running_num += 1\n",
    "    return caption\n",
    "\n",
    "def getPageText(pdf_file, index):\n",
    "    caption=[]\n",
    "    # zoom = 1.2 # scale your pdf file by 120%\n",
    "    # mat = fitz.Matrix(zoom, zoom)\n",
    " \n",
    "    page = pdf_file[index]\n",
    "    text = page.get_text()\n",
    "    caption = getCaption(text)\n",
    "    return caption\n",
    "\n",
    "def isWholeNumber(NumCaption,NumImage): ##Image caption calculation, only 1 or 2 image with captions will be saved\n",
    "    if(NumImage % NumCaption ==0):\n",
    "        result = ('It is a whole number. - '+ str(NumImage % NumCaption))\n",
    "        return True\n",
    "    else:\n",
    "        result = ('It not a whole number, having a remainder of. - '+ str(NumImage % NumCaption))\n",
    "        return False\n",
    "\n",
    "# Function to maintain FileMetadata table (Update Image caption DB - vincent code dun have this)\n",
    "def InsertUpdateFileMetadata_ImageCaption(config_dict, cursor):\n",
    "    # Query to get count of existing rows in FileMetadata table \n",
    "    row = cursor.execute(\"SELECT count(*), max(Id) as count FROM [dbo].[FileMetadata_ImageCaption] WHERE FileMetadataId = ?\", config_dict['FileMetadataId']).fetchone()\n",
    "       \n",
    "    rows_returned = row[1]\n",
    "    if (rows_returned is None): \n",
    "        # Insert new record \n",
    "        cursor.execute(\"INSERT INTO dbo.FileMetadata_ImageCaption(FileMetadataId,opu,pageNumber,blobPath,caption,createddate,isDeleted) values(?,?,?,?,?,getdate(),?);\", config_dict['FileMetadataId'], config_dict['opu'], config_dict['pageNum'], config_dict['blobPath'], config_dict['caption'], config_dict['isDeleted'])\n",
    "        JDBCconnection.commit() \n",
    "        # row = cursor.execute(\"SELECT IDENT_CURRENT('[dbo].[FileMetadata]');\").fetchone()\n",
    "        return str(rows_returned)\n",
    "    else:        \n",
    "        # Update existing record\n",
    "        cursor.execute(\"UPDATE [dbo].[FileMetadata_ImageCaption] SET opu = ?, pageNumber = ? ,blobPath = ?, caption = ?,isDeleted = ?, ModifiedDate = getdate() WHERE FileMetadataId = ?;\", config_dict['opu'],config_dict['pageNum'], config_dict['blobPath'], config_dict['caption'], config_dict['isDeleted'], config_dict['FileMetadataId'])\n",
    "        JDBCconnection.commit() \n",
    "        return (str(rows_returned))\n",
    "\n",
    "\n",
    "def updateCaption(caption,img_count,imagelist):\n",
    "    cursor = JDBCconnection.cursor()\n",
    "    isWholeNumberResult = isWholeNumber(len(caption),img_count)\n",
    "    if(isWholeNumberResult==True):\n",
    "        image_df=pd.DataFrame(imagelist,columns=['FileMetadataId','sourceTable','pageNum','blobPath','isDeleted'])\n",
    "        #convert caption list into dataframe\n",
    "        caption_df = pd.DataFrame(caption,columns=['caption'])\n",
    "        #add a key column in caption and image datafrme \n",
    "        caption_df['key']=1\n",
    "        image_df['key']=1\n",
    "        #check the count number is 2 or 0, 2 is image count > caption count, 0 is image count = caption count\n",
    "        count = int(len(imagelist)/len(caption))\n",
    "        print('count: '+ str(count))\n",
    "\n",
    "        if(count==1 or count == 2):\n",
    "            # replicate the caption to match with image count\n",
    "            num_replications = int(len(image_df)/len(caption)-1)\n",
    "            for _ in range(num_replications):\n",
    "                caption_df = pd.concat([caption_df,caption_df], ignore_index=True)\n",
    "            #set accending order for the caption\n",
    "            caption_df = caption_df.sort_values('caption', ascending=True)\n",
    "\n",
    "            # set the sequance key value\n",
    "            key = 1\n",
    "            new_df=[]\n",
    "            for index, i in image_df.iterrows():\n",
    "                i['key'] = key\n",
    "                new_df.append(i)\n",
    "                key += 1\n",
    "            image_df = pd.DataFrame(new_df)\n",
    "\n",
    "            key = 1\n",
    "            new_df=[]\n",
    "            for index, i in caption_df.iterrows():\n",
    "                i['key'] = key\n",
    "                new_df.append(i)\n",
    "                key += 1\n",
    "            caption_df = pd.DataFrame(new_df)\n",
    "            #merge the caption and image dataframe\n",
    "            final_df = image_df.merge(caption_df, on=['key'])\n",
    "            final_df = final_df.drop('key',axis=1)\n",
    "\n",
    "            #insert the caption into FileMetadata_ImageCaption\n",
    "            for index,i in final_df.iterrows():\n",
    "                captionconfig_dict={\n",
    "                    'FileMetadataId':i['FileMetadataId'],\n",
    "                    'blobPath':i['blobPath'],\n",
    "                    'pageNum':i['pageNum'],\n",
    "                    'caption':i['caption'],\n",
    "                    'isDeleted':i['isDeleted'],\n",
    "                    'opu':i['sourceTable']\n",
    "                }\n",
    "                print(captionconfig_dict)\n",
    "                \n",
    "                InsertUpdateFileMetadata_ImageCaption(captionconfig_dict,cursor)\n",
    "                print('Updated db successfully')\n",
    "                #JDBCconnection.commit() \n",
    "    else:\n",
    "        print('is not whole number')\n",
    "\n",
    "def isGeneratedPDF(result,numpage,ans):\n",
    "    #check the 5 pages result \n",
    "    count = 0\n",
    "    if ans == 'yes':\n",
    "        return True\n",
    "    for i in result:\n",
    "        if(i == \"This page may be a generated PDF.\"):\n",
    "            count += 1\n",
    "    if(count >= 3 and numpage>=5):\n",
    "        return True \n",
    "    elif(count >= 2 and numpage==4):\n",
    "        return True\n",
    "    elif(count >= 1 and numpage==3):\n",
    "        return True\n",
    "    elif(count == 1 and numpage==2):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_plus_minus_2(n1, n2, n3, n4):\n",
    "    #size = page - image\n",
    "    #page - image, if negative then is scanned doc\n",
    "    width = n2 - n1\n",
    "    height = n4 - n3\n",
    "\n",
    "    if width <= 0 and height <= 0:\n",
    "        result=\"This page may be a scanned PDF.\"\n",
    "    elif width > 400 and height > 400:\n",
    "         result=\"This page may be a scanned PDF.\"\n",
    "    else:\n",
    "        result=\"This page may be a generated PDF.\"\n",
    "    return result\n",
    "\n",
    "#Incremental code\n",
    "def is_incremental(cursor,opu,docType):\n",
    "    source_str = source.split('_')[0]\n",
    "    container_schema = 'EKIH'\n",
    "    \n",
    "    query = \"SELECT Is_Incremental FROM [dbo].[ingestion_enigma_config_image_extraction] WHERE Source_Table_Name = '{}' AND Doc_Type = '{}' and Source = '{}'\".format(opu,docType.upper(),source_str)\n",
    "    result = cursor.execute(query).fetchone()\n",
    "    \n",
    "    if result is None:\n",
    "        inc_val = '0'\n",
    "\n",
    "        cursor.execute(\"INSERT INTO [dbo].[ingestion_enigma_config_image_extraction] (Is_Incremental,Container_Schema,Source_Table_Name,Is_active,Source,Doc_Type) values(?,?,?,'1',?,?);\",inc_val,container_schema,opu,source_str,docType.upper())\n",
    "        JDBCconnection.commit()\n",
    "        return inc_val \n",
    "    else:\n",
    "        return result[0]\n",
    "\n",
    "def latest_runtime(cursor,opu):\n",
    "    \n",
    "    execution_end_time_query = \"SELECT Top(1) ExecutionEndTime FROM [dbo].[ingestion_log_image_extraction] WHERE OPU = '{}' and ExecutionType = 'Image Extraction' ORDER BY ExecutionStartTime DESC\".format(opu)\n",
    "\n",
    "    execution_end_time = cursor.execute(execution_end_time_query).fetchone()\n",
    "\n",
    "    return execution_end_time[0]\n",
    "\n",
    "def update_incremental(cursor,opu,docType):\n",
    "    \n",
    "    cursor.execute(\"UPDATE [dbo].[ingestion_enigma_config_image_extraction] set Is_Incremental='1' where Source_Table_Name='{}' AND Doc_Type = '{}'\".format(opu, docType.upper()))\n",
    "    JDBCconnection.commit()\n",
    "        \n",
    "    print('\\nUpdated Incremental DB successfully')\n",
    "\n",
    "\n",
    "def get_blob(container, path, docType, docFormat):\n",
    "    service_client = DataLakeServiceClient(account_url = \"https://{0}.dfs.core.windows.net/\".format(storage_acc), credential=token_credential)\n",
    "    file_system_client = service_client.get_file_system_client(file_system=container) \n",
    "    paths = file_system_client.get_paths(path)   \n",
    "    files = [i.name for i in paths]\n",
    "    file_list = []\n",
    "    for i in files:\n",
    "        if docType.upper() in i.upper() and docFormat.upper() in i.upper():\n",
    "            file_list.append(i)\n",
    "    return file_list\n",
    "\n",
    "def path_validation(cursor, folder_paths, source, opu, docType, docFormat):\n",
    "\n",
    "    if source.rsplit('_', 1)[0] != 'PEDMS':\n",
    "        if source.rsplit('_', 1)[0] == 'ECMS':#FOR SKILL and ECMS\n",
    "            new_opu = docType\n",
    "            docType = ''\n",
    "        else:\n",
    "            new_opu=''\n",
    "\n",
    "        file_id_query = \"SELECT ID FROM [dbo].[FileMetadata] WHERE Source LIKE '%{}%' and SourceTableName LIKE '%{}%' and FileName LIKE '%{}%' and FileType LIKE '%{}%' and IsDeleted='0' and IsActive ='1'\".format(source.rsplit('_', 1)[0], new_opu, docType, docFormat.lower())\n",
    "        file_id = cursor.execute(file_id_query).fetchall()\n",
    "        \n",
    "    else: #THIS IS FOR PEDMS\n",
    "        file_id_query = \"SELECT ID FROM [dbo].[FileMetadata] meta INNER join [dbo].[PEDMS_OBID_Metadata] OBID ON meta.IDColumnValue = OBID.OBID and OBID.opu=meta.SourceTableName and meta.source ='{}' and OBID.opu='{}' and is_deleted='false' and OBID.Doc_Type_Name  ='{}' and meta.isactive = 1 and meta.isdeleted = 0 WHERE meta.source ='{}' and OBID.opu='{}' and OBID.Doc_Type_Name  ='{}' and meta.isactive = 1 and meta.isdeleted = 0\".format(source.rsplit('_', 1)[0],opu,docType,source.rsplit('_', 1)[0],opu,docType)\n",
    "        file_id = cursor.execute(file_id_query).fetchall()\n",
    "\n",
    "    new_list = [name[0] for name in file_id]\n",
    "\n",
    "    fileList_df = pd.DataFrame(new_list, columns=['ID'])\n",
    "    folderPaths_df = pd.DataFrame(folder_paths,columns=['Path'])\n",
    "\n",
    "    folderPaths_df['PathID'] = folderPaths_df['Path'].str.split('_',expand=True)[0].astype(int)\n",
    "\n",
    "    filtered_folderPaths_df = folderPaths_df.merge(fileList_df, left_on='PathID', right_on='ID', how='inner')\n",
    "\n",
    "    latest_folderPaths = filtered_folderPaths_df['Path'].tolist()\n",
    "\n",
    "    return latest_folderPaths\n",
    "\n",
    "def retrieve_file_names_from_sql_log(source,opu,docType):\n",
    "    cursor = JDBCconnection.cursor()\n",
    "    source_str = source.split('_')[0]\n",
    "\n",
    "    if source_str == 'ECMS':\n",
    "        docType = docType + '.'\n",
    "\n",
    "    query = \"SELECT SourceFileName FROM [dbo].[ingestion_log_image_extraction] WHERE Status = 'Succeeded' AND ExecutionType = 'Image Extraction' AND Source like '%{}%' AND OPU = '{}' AND SourceFileName LIKE '%{}%' \".format(source_str, opu,docType)\n",
    "    result = cursor.execute(query).fetchall()\n",
    "\n",
    "    result_list = [name[0] for name in result]\n",
    "    df = pd.DataFrame({\"File Name\": result_list})\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    new_list = df['File Name'].values.tolist()\n",
    "\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8378a91-cef6-4fc7-ac85-047ca1114d45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## TO GET FILEPATH\n",
    "path = \"AZ_ADLS_PR_UNSTRUCTURED/{0}/{1}/V01/\".format(source, opu)\n",
    "process_fileSystem = 'process'\n",
    "input_directory = \"abfss://{0}@{1}.dfs.core.windows.net/AZ_ADLS_PR_UNSTRUCTURED/{2}/{3}/V01/\".format(process_fileSystem,storage_acc,source,opu)\n",
    "cursor = JDBCconnection.cursor()\n",
    "\n",
    "documentFormat = ['PDF','DOC']\n",
    "latest_folderPaths = []\n",
    "for docFormat in documentFormat:\n",
    "    incremental = is_incremental(cursor,opu,docType)\n",
    "    print(incremental)\n",
    "    print(docFormat)\n",
    "    folder_paths=[]\n",
    "    temp_folderPaths = []\n",
    "\n",
    "    if incremental == '0':\n",
    "    #proceed with normal load - Update Is_Incremental to 1 (so it will load incrementally in the future)        \n",
    "        file_list = dbutils.fs.ls(input_directory)\n",
    "\n",
    "        for folder in file_list:\n",
    "            if docType.upper() in folder.name.upper() and docFormat.upper() in folder.name.upper():\n",
    "                folder_paths.append(folder.name)\n",
    "\n",
    "        if (len(folder_paths))>0:\n",
    "            try:\n",
    "                temp_folderPaths = path_validation(cursor, folder_paths, source, opu, docType,docFormat) #based on filemetadata\n",
    "                print(len(temp_folderPaths))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        if (len(temp_folderPaths))>0:\n",
    "            extracted_files = retrieve_file_names_from_sql_log(source,opu,docType)\n",
    "            print(len(extracted_files))\n",
    "            new_list = [x for x in temp_folderPaths if x not in extracted_files]\n",
    "            print(len(new_list))\n",
    "            print(new_list)\n",
    "            latest_folderPaths.append(new_list)\n",
    "        else:\n",
    "            print('There is no related documents to be executed!')\n",
    "\n",
    "    else:\n",
    "        file_list = get_blob(process_fileSystem, path, docType,docFormat)\n",
    "        try:\n",
    "            latest_time = latest_runtime(cursor,opu).timestamp()\n",
    "        except:\n",
    "            latest_time = int(datetime.fromtimestamp(0).timestamp())\n",
    "        file_list_modified_date = []\n",
    "\n",
    "        for i in file_list:\n",
    "            storage_account_url=\"{}://{}.dfs.core.windows.net\".format(\"https\",storage_acc)\n",
    "            service_client = DataLakeServiceClient(account_url=storage_account_url, credential=token_credential)\n",
    "            directory_client = service_client.get_directory_client(process_fileSystem, i)\n",
    "            directory_properties = directory_client.get_directory_properties()\n",
    "            if directory_properties['last_modified'].timestamp() > latest_time:\n",
    "                paths = os.path.basename(directory_properties['name'])\n",
    "                folder_paths.append(paths)\n",
    "\n",
    "        if len(folder_paths)>0:\n",
    "            try:\n",
    "                temp_folderPaths = path_validation(cursor, folder_paths, source, opu, docType, docFormat)\n",
    "                print(len(temp_folderPaths))\n",
    "                latest_folderPaths.append(temp_folderPaths)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            print('There is no related documents to be executed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5223fe51-efee-42cc-910c-91c44f1c87aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DocumentCracker():\n",
    "    def __init__(self, source_client, target_adls_container):\n",
    "        self.source_client = source_client\n",
    "        self.target_adls_container = target_adls_container\n",
    "        self.file_properties = source_client.get_file_properties()\n",
    "        self.file_path = source_client.get_file_properties()['name']\n",
    "        self.file_type = self.file_path.rsplit('.',1)[1].lower()\n",
    "        self.extracted_images_hash = []\n",
    "        self.images_extracted = 0\n",
    "        self.images_extracted_failed = 0\n",
    "        self.error_description = ''\n",
    "        self.cursor = JDBCconnection.cursor()\n",
    "        self.filemetadata_details = self.get_file_metadata_details(self.file_properties['metadata']['file_metadata_id'])\n",
    "        \n",
    "    def get_image_format(self, image_type):\n",
    "        return {\n",
    "            \"jpeg\": drawing.imaging.ImageFormat.jpeg,\n",
    "            \"emf\": drawing.imaging.ImageFormat.emf,\n",
    "            \"bmp\": drawing.imaging.ImageFormat.bmp,\n",
    "            \"png\": drawing.imaging.ImageFormat.png,\n",
    "            \"wmf\": drawing.imaging.ImageFormat.wmf,\n",
    "            \"gif\": drawing.imaging.ImageFormat.gif,\n",
    "        }.get(image_type, drawing.imaging.ImageFormat.jpeg)\n",
    "    \n",
    "    def log_start(self, log_details):\n",
    "        try:\n",
    "            self.cursor.execute(\"INSERT INTO [dbo].[ingestion_log_image_extraction](Source, SourceFileName, PipelineRunId, Status, ExecutionStartTime, OPU, ExecutionType) VALUES (?, ?, ?, 'In Progress', getdate(), ?, 'Image Extraction');\", log_details['source'], log_details['source_file_name'], log_details['pipeline_run_id'],opu)\n",
    "            log_id = self.cursor.execute(\"SELECT IDENT_CURRENT('[dbo].[ingestion_log_image_extraction]');\").fetchone()[0]\n",
    "            JDBCconnection.commit()\n",
    "            return log_id\n",
    "        except:\n",
    "            print(str(traceback.format_exc()))\n",
    "            return 'failed'\n",
    "        \n",
    "    def log_end(self, log_details):\n",
    "        try:\n",
    "            self.cursor.execute(\"UPDATE [dbo].[ingestion_log_image_extraction] set Status = ?, ErrorDescription = ?, ExecutionEndTime = getdate(), ImagesExtracted = ?, ImagesExtractFailed = ?, OPU = ? where ID = ?;\", log_details['status'], log_details['error_description'], log_details['image_extracted'], log_details['image_extracted_failed'],log_details['opu'], log_details['log_id'])\n",
    "            JDBCconnection.commit()\n",
    "            return 'succeeded' \n",
    "        except:\n",
    "            print(str(traceback.format_exc()))\n",
    "            return 'failed'\n",
    "        \n",
    "    def get_target_file_client(self, file_path_name):\n",
    "        return DataLakeFileClient(\n",
    "            account_url=account_url, \n",
    "            file_system_name = self.target_adls_container, \n",
    "            file_path=file_path_name, \n",
    "            credential=token_credential\n",
    "        )\n",
    "        \n",
    "    def get_file_metadata_details(self, filemetadata_id):\n",
    "        try:\n",
    "            query = \"SELECT Source, SourceDatabase, SourceTableName FROM FileMetadata WHERE ID = '{}'\".format(filemetadata_id)\n",
    "            result = cursor.execute(query).fetchone()\n",
    "            metadata_properties = {\n",
    "                'Source': result[0],\n",
    "                'SourceDatabase': result[1],\n",
    "                'SourceTableName': result[2]\n",
    "            }\n",
    "            return metadata_properties\n",
    "        except:\n",
    "            print('metadata not found for: ' + self.file_path)\n",
    "            #raise SystemExit\n",
    "    \n",
    "    def update_filemetadata(self, **details):        \n",
    "        query = \"SET NOCOUNT ON \\\n",
    "            DECLARE @ChangeResult TABLE (Id INTEGER)\\\n",
    "            MERGE INTO [dbo].[FileMetadata] AS TargetTbl USING (VALUES\\\n",
    "            ('{SourceDatabase}', '{SourceTableName}', 'FileMetadata_ID', '{IDColumnValue}', '{FileName}', '{FileType}', '{BlobPath}', '{Source}')\\\n",
    "            ) AS SourceTbl (SourceDatabase, SourceTableName, IDColumnName, IDColumnValue, FileName, FileType, BlobPath, Source)\\\n",
    "            ON (TargetTbl.Source = SourceTbl.Source AND TargetTbl.IDColumnValue = SourceTbl.IDColumnValue AND substring(TargetTbl.FileName, charindex('_', TargetTbl.FileName) + 1 , len(TargetTbl.FileName)) = SourceTbl.FileName AND TargetTbl.IsActive = 1)\\\n",
    "            WHEN MATCHED THEN\\\n",
    "            UPDATE SET\\\n",
    "            FileName = concat(TargetTbl.ID, '_', SourceTbl.FileName), FileType = SourceTbl.FileType, BlobPath = concat(SourceTbl.BlobPath,TargetTbl.ID,'_',SourceTbl.FileName), ModifiedDate = getdate(), IsDeleted = 0\\\n",
    "            WHEN NOT MATCHED BY TARGET THEN\\\n",
    "            INSERT (SourceDatabase,SourceTableName,IDColumnName,IDColumnValue,FileType,IsActive,CreatedDate, FileName, BlobPath, Source, IsDeleted) values\\\n",
    "            (SourceTbl.SourceDatabase,SourceTbl.SourceTableName,SourceTbl.IDColumnName,SourceTbl.IDColumnValue,SourceTbl.FileType, 1, getdate() ,(select concat((select IDENT_CURRENT('[dbo].[FileMetadata]')),'_', SourceTbl.FileName)) ,(select concat(SourceTbl.BlobPath, (select IDENT_CURRENT('[dbo].[FileMetadata]')),'_', SourceTbl.FileName)), SourceTbl.Source, 0)\\\n",
    "            OUTPUT inserted.Id INTO @ChangeResult;\\\n",
    "            SELECT * FROM @ChangeResult\"\\\n",
    "            .format(SourceDatabase=details['SourceDatabase'],\n",
    "            SourceTableName=details['SourceTableName'],\n",
    "            IDColumnValue=details['IDColumnValue'],\n",
    "            FileName=details['FileName'],\n",
    "            FileType=details['FileType'],\n",
    "            BlobPath=details['BlobPath'],\n",
    "            Source=details['Source'])\n",
    "        metadata_id = self.cursor.execute(query).fetchone()[0]\n",
    "        JDBCconnection.commit()\n",
    "        return str(metadata_id)\n",
    "\n",
    "    def validate_image(self, image_bytes):\n",
    "        if hashlib.md5(image_bytes).digest() not in self.extracted_images_hash and len(image_bytes) > 0 and magic.from_buffer(bytes(image_bytes), mime=True).split('/',1)[0] == 'image':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def crack_document(self):\n",
    "        # Log Progress\n",
    "        log_details = {\n",
    "            \"source\": self.filemetadata_details['Source'], \n",
    "            \"source_file_name\":os.path.basename(self.file_path),\n",
    "            \"pipeline_run_id\": pipelineRunID\n",
    "        }\n",
    "        log_id = self.log_start(log_details)\n",
    "        \n",
    "        # Extract images based on file type \n",
    "        if self.file_type in ['pdf']:\n",
    "            self.crack_pdf()\n",
    "        elif self.file_type in ['docx', 'doc']:\n",
    "            self.crack_docx()\n",
    "        elif self.file_type in ['pptx', 'ppt']:\n",
    "            self.crack_pptx()\n",
    "            \n",
    "        if self.images_extracted_failed < 1:\n",
    "            log_details = {\n",
    "                \"status\": 'Succeeded', \n",
    "                \"error_description\":None,\n",
    "                \"image_extracted\": self.images_extracted,\n",
    "                \"image_extracted_failed\": self.images_extracted_failed,\n",
    "                \"log_id\": log_id,\n",
    "                \"opu\": opu\n",
    "            }\n",
    "        else:\n",
    "            log_details = {\n",
    "                \"status\": 'Failed', \n",
    "                \"error_description\": self.error_description if self.images_extracted_failed < 1 else (\"Error extracting certain images. \" + self.error_description),\n",
    "                \"image_extracted\": self.images_extracted,\n",
    "                \"image_extracted_failed\": self.images_extracted_failed,\n",
    "                \"log_id\": log_id,\n",
    "                \"opu\": opu\n",
    "            }\n",
    "        self.log_end(log_details)   \n",
    "            \n",
    "        # Close Connection\n",
    "        self.cursor.close()\n",
    "\n",
    "    def crack_pdf(self):\n",
    "        generated_count=0\n",
    "        try:\n",
    "            ## Oscar's detection of Scanned/Generated Documents\n",
    "            raw_content = fileClient_source.download_file().readall() #Download file into bytes\n",
    "\n",
    "            result,numpage,ans= PDFPageCheck(raw_content)\n",
    "            \n",
    "            #identify the pdf file is generated or scanned  \n",
    "            isGenerated = isGeneratedPDF(result,numpage,ans)\n",
    "\n",
    "            if(isGenerated is True):\n",
    "                print('\\n' + self.file_path+'- Generated file : '+ str(isGenerated)) \n",
    "                pdf_file = fitz.open(stream=raw_content, filetype=\"pdf\")\n",
    "\n",
    "                ## Oscar's detection of Scanned/Generated Documents\n",
    "                img_index = 1\n",
    "                filemetadata_id = self.file_properties['metadata']['file_metadata_id']\n",
    "                \n",
    "                caption=[]\n",
    "\n",
    "                # iterating through each page in the pdf\n",
    "                for current_page_index in range(len(pdf_file)):\n",
    "                    img_count=0\n",
    "                    caption = getPageText(pdf_file,current_page_index) #get caption of images in the PDF file\n",
    "                    imagelist=[]\n",
    "                    # iterating through each image in every page of PDF\n",
    "                    for _ ,img in enumerate(pdf_file.get_page_images(current_page_index)):\n",
    "                        xref = img[0]\n",
    "                        image = fitz.Pixmap(pdf_file, xref)\n",
    "                        image_filename = \"{}_extracted image_{}.png\".format(filemetadata_id, img_index)\n",
    "                        #image = Image.open(image_file)\n",
    "                        image_width = image.width\n",
    "                        image_height = image.height\n",
    "\n",
    "                        # if it is CMYK: convert to RGB first\n",
    "                        if image.n >= 5:   #5                  \n",
    "                            image = fitz.Pixmap(fitz.csRGB, image)\n",
    "\n",
    "                        image_bytes = image.tobytes()\n",
    "                        \n",
    "                        if self.validate_image(image_bytes) and image_width >= 380 and image_height >= 230:\n",
    "                            try:\n",
    "                                # Update Metadata \n",
    "                                new_metadata_id = self.update_filemetadata(\n",
    "                                    SourceDatabase=self.filemetadata_details['SourceDatabase'],\n",
    "                                    SourceTableName=self.filemetadata_details['SourceTableName'],\n",
    "                                    IDColumnValue=filemetadata_id,\n",
    "                                    FileName=os.path.basename(image_filename),\n",
    "                                    FileType= '.' + image_filename.rsplit('.',1)[1],\n",
    "                                    BlobPath=os.path.dirname(self.file_path) + '/',\n",
    "                                    Source=self.filemetadata_details['Source']\n",
    "                                )\n",
    "                               \n",
    "                                image_filename = (\"{}/{}_\".format(os.path.dirname(self.file_path), new_metadata_id) + image_filename).replace('//', '/')\n",
    "                                print(image_filename)\n",
    "                                md5_hash = hashlib.md5(image_bytes).digest()\n",
    "                                self.extracted_images_hash.append(md5_hash)\n",
    "                                img_count += 1\n",
    "                                print(\"\\nSuccessfully saved png:\", image_filename)\n",
    "                                \n",
    "                                # Upload to ADLS \n",
    "                                target_client = self.get_target_file_client(image_filename)\n",
    "                                metadata = {'file_metadata_id':new_metadata_id, 'classification':self.file_properties['metadata']['classification']}\n",
    "                                \n",
    "                                # Get content type \n",
    "                                processedContentType = magic.from_buffer(image_bytes, mime=True)\n",
    "                                \n",
    "                                # Specify ContentType for file \n",
    "                                fileContent = ContentSettings(content_type=processedContentType, content_md5=md5_hash)\n",
    "                                target_client.upload_data(image_bytes, overwrite=True, metadata=metadata, content_settings = fileContent)\n",
    "\n",
    "                                #test\n",
    "                                #filename = getImageFileMetadataId (metadata , cursor , Source) \n",
    "                                #stored the image data into imagelist\n",
    "                                #imagelist.append([metadata,filename,len(pdf_file)+1,blobPath+metadata+image_filename,isDeleted])\n",
    "                                isDeleted = 'False'\n",
    "                                #imagelist.append([new_metadata_id,self.filemetadata_details['SourceTableName'],len(pdf_file)+1,image_filename,isDeleted])\n",
    "                                imagelist.append([new_metadata_id,opu,len(pdf_file)+1,image_filename,isDeleted])\n",
    "                                \n",
    "                                self.images_extracted = img_index\n",
    "                                img_index += 1\n",
    "                                del image_bytes\n",
    "                        \n",
    "                            except Exception as e:\n",
    "                                self.images_extracted_failed += 1 \n",
    "                                print(str(traceback.format_exc()))\n",
    "                        else:\n",
    "                            #self.images_extracted += 1\n",
    "                            del image_bytes \n",
    "\n",
    "                    #Include caption generation and will be updated to DB\n",
    "                    if(img_count>0 and len(caption)>0):\n",
    "                        updateCaption(caption, img_count, imagelist)\n",
    "\n",
    "            else:\n",
    "                print('\\n' + self.file_path+'- Generated file : '+ str(isGenerated)) \n",
    "                #print('\\n' + self.file_path + \" is a scanned document\")\n",
    "\n",
    "        except:\n",
    "            self.error_description += str(traceback.format_exc()) + ' || ' + self.error_description\n",
    "            print(self.error_description)\n",
    "            \n",
    "    def crack_docx(self):\n",
    "        try:\n",
    "            exclude_aspose_image = b'\\xc0<\\xd7r\\xc9\\x16\\xd9<\\xb3g\\xd8\\xfe\\xdc\\x98\\xb0\\xec'\n",
    "            print(\"\\nProcessing DOC/DOCX\", self.file_path)\n",
    "\n",
    "            # load the Word document\n",
    "            streamdownloader=self.source_client.download_file()\n",
    "            streamdownloader_bytes = streamdownloader.readall()\n",
    "            doc = aw.Document(BytesIO(streamdownloader_bytes))\n",
    "            filemetadata_id = self.file_properties['metadata']['file_metadata_id']\n",
    "            \n",
    "            # create a list of the first 5 pages\n",
    "            # pages = [document.sections[0].header, document.sections[0].footer, document.sections[0].first_page_header, document.sections[0].first_page_footer, document.sections[0].page]\n",
    "            #print(pages)\n",
    "\n",
    "            # retrieve all shapes\n",
    "            shapes = doc.get_child_nodes(aw.NodeType.SHAPE, True)\n",
    "            imageIndex = 1\n",
    "\n",
    "            # loop through shapes\n",
    "            for shape in shapes :\n",
    "                shape = shape.as_shape()\n",
    "\n",
    "                if shape.has_image:\n",
    "                    shape_bytes = shape.image_data.image_bytes\n",
    "                    shape_hash = hashlib.md5(shape_bytes).digest()\n",
    "\n",
    "                    shape_width = shape.width\n",
    "                    shape_height = shape.height\n",
    "                    #print(shape_width, shape_height)\n",
    "\n",
    "                    if self.validate_image(shape_bytes) and shape_hash != exclude_aspose_image and shape_width >380 and shape_height > 230:\n",
    "                        try:\n",
    "                            # set image file's name\n",
    "                            image_filename = \"{}_extracted image_{}.png\".format(filemetadata_id, imageIndex)\n",
    "\n",
    "                            # Update Metadata \n",
    "                            new_metadata_id = self.update_filemetadata(\n",
    "                                SourceDatabase=self.filemetadata_details['SourceDatabase'],\n",
    "                                SourceTableName=self.filemetadata_details['SourceTableName'],\n",
    "                                IDColumnValue=filemetadata_id,\n",
    "                                FileName=os.path.basename(image_filename),\n",
    "                                FileType= '.' + image_filename.rsplit('.',1)[1],\n",
    "                                BlobPath=os.path.dirname(self.file_path) + '/',\n",
    "                                Source=self.filemetadata_details['Source']\n",
    "                            )\n",
    "                            \n",
    "                            #image_filename = \"{}/testing/{}_\".format(os.path.dirname(self.file_path), new_metadata_id) + image_filename\n",
    "                            image_filename = (\"{}/{}_\".format(os.path.dirname(self.file_path), new_metadata_id) + image_filename).replace('//', '/')\n",
    "\n",
    "                            # save image\n",
    "                            self.extracted_images_hash.append(shape_hash)\n",
    "                            print(\"Successfully saved png:\", image_filename)\n",
    "                            \n",
    "                            target_client = self.get_target_file_client(image_filename)\n",
    "                            # Get content type \n",
    "                            processedContentType = magic.from_buffer(bytes(shape_bytes), mime=True)\n",
    "                            # Specify ContentType for file \n",
    "                            fileContent = ContentSettings(content_type=processedContentType, content_md5=shape_hash)\n",
    "                            metadata = {'file_metadata_id':new_metadata_id, 'classification':self.file_properties['metadata']['classification']}\n",
    "                            target_client.upload_data(bytes(shape_bytes), overwrite=True, metadata=metadata, content_settings = fileContent)\n",
    "                            self.images_extracted = imageIndex\n",
    "                            imageIndex += 1\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            self.images_extracted_failed += 1 \n",
    "                            print(traceback.format_exc())\n",
    "                        else:\n",
    "                            del image_bytes\n",
    "        except:\n",
    "            self.error_description += str(traceback.format_exc()) + ' || ' + self.error_description\n",
    "            \n",
    "    def crack_pptx(self):\n",
    "        try:\n",
    "            print(\"\\nProcessing PPT/PPTX\", self.file_path)\n",
    "            imageIndex=0\n",
    "            # load presentation\n",
    "            filemetadata_id = self.file_properties['metadata']['file_metadata_id']\n",
    "            streamdownloader=self.source_client.download_file()\n",
    "            streamdownloader_bytes = streamdownloader.readall()\n",
    "            with slides.Presentation(BytesIO(streamdownloader_bytes)) as pres:\n",
    "\n",
    "                # loop through images\n",
    "                for image in pres.images:\n",
    "                    image_type = image.content_type.split(\"/\")[1]\n",
    "                    image_bytes = image.binary_data\n",
    "                    \n",
    "                    #image_width = image.width\n",
    "                    #print(\"width: \", image_width)\n",
    "                    image_width = image.width\n",
    "                    image_height = image.height\n",
    "\n",
    "                    if self.validate_image(image_bytes) and image_width > 380 and image_height >230 and image_type.lower() in ['jpg', 'jpeg', 'png', 'gif', 'bmp']:\n",
    "                        try:\n",
    "                            image_filename = \"{}_extracted image_{}.{}\".format(filemetadata_id, imageIndex, image_type)\n",
    "                            image_format = self.get_image_format(image_type)\n",
    "                            md5_hash = hashlib.md5(image_bytes).digest()\n",
    "\n",
    "                            # Update Metadata \n",
    "                            new_metadata_id = self.update_filemetadata(\n",
    "                                SourceDatabase=self.filemetadata_details['SourceDatabase'],\n",
    "                                SourceTableName=self.filemetadata_details['SourceTableName'],\n",
    "                                IDColumnValue=filemetadata_id,\n",
    "                                FileName=os.path.basename(image_filename),\n",
    "                                FileType= '.' + image_filename.rsplit('.',1)[1],\n",
    "                                BlobPath=os.path.dirname(self.file_path) + '/',\n",
    "                                Source=self.filemetadata_details['Source']\n",
    "                            )\n",
    "                            \n",
    "                            image_filename = \"{}/{}_\".format(os.path.dirname(self.file_path), new_metadata_id) + image_filename\n",
    "\n",
    "                            # save image\n",
    "                            self.extracted_images_hash.append(md5_hash)\n",
    "                            imageIndex += 1\n",
    "\n",
    "                            target_client = self.get_target_file_client(image_filename)\n",
    "                            # Get content type \n",
    "                            processedContentType = magic.from_buffer(bytes(image_bytes), mime=True)\n",
    "                            # Specify ContentType for file \n",
    "                            fileContent = ContentSettings(content_type=processedContentType, content_md5=md5_hash)\n",
    "                            metadata = {'file_metadata_id':new_metadata_id, 'classification':self.file_properties['metadata']['classification']}\n",
    "                            target_client.upload_data(bytes(image_bytes), overwrite=True, metadata=metadata, content_settings = fileContent)\n",
    "                            print(\"Successfully saved png:\", image_filename)\n",
    "                        except Exception as e:\n",
    "                            self.images_extracted_failed += 1 \n",
    "                            print(traceback.format_exc())\n",
    "                        else:\n",
    "                            slf.images_extracted += 1 \n",
    "        except:\n",
    "            self.error_description += str(traceback.format_exc()) + ' || ' + self.error_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53d96149-bf3f-46bf-a6c0-f17bd50ee10d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = '/AZ_ADLS_PR_UNSTRUCTURED/{0}/{1}/V01/'.format(source,opu) #These are for folder_path loop\n",
    "\n",
    "latest_folderPaths = [item for sublist in latest_folderPaths for item in (sublist if isinstance(sublist, list) else sublist)]\n",
    "cursor = JDBCconnection.cursor()\n",
    "\n",
    "if (len(latest_folderPaths)) > 0:\n",
    "    for path_number in range(len(latest_folderPaths)):\n",
    "        full_path = path+str(latest_folderPaths[path_number])\n",
    "\n",
    "        fileClient_source = DataLakeFileClient(\n",
    "            account_url=account_url, \n",
    "            file_system_name ='process', \n",
    "            file_path=full_path,\n",
    "            credential=token_credential\n",
    "        )\n",
    "\n",
    "        try: \n",
    "            pipelineRunID \n",
    "        except:\n",
    "            pipelineRunID = '00000000-0000-0000-0000-000000000000'\n",
    "\n",
    "        if fileClient_source.exists():\n",
    "            document_cracker = DocumentCracker(fileClient_source, 'process')\n",
    "            try:\n",
    "                document_cracker.crack_document()\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            print(\"File not exists\")\n",
    "\n",
    "    update_incremental(cursor,opu,docType)\n",
    "\n",
    "else:\n",
    "    update_incremental(cursor,opu,docType)\n",
    "    print('There are no documents to be executed')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Image_Extraction_Caption_Generation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
